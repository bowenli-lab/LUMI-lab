{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdl/miniconda3/envs/unimol/lib/python3.9/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import wandb\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from modAL.disagreement import max_std_sampling\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from unicore.optim.fused_adam import FusedAdam\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "from unimol.hf_unimol import UniMol, UniMolConfig, init_unimol_backbone\n",
    "from unimol.hg_mterics import compute_metrics\n",
    "from unimol.lmdb_dataset import collate_fn, load_dataset\n",
    "from learners import ActiveLearner, CommitteeRegressor\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# set wandb offline\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the point sampling with ensemble pytorch model first\n",
    "\n",
    "1. put the models in the learner\n",
    "2. Update and sample the data points\n",
    "3. Evaluate the model with new metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/fs01/home/haotian/SDL-LNP/model/unimol/dict.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m      9\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/datasets/cellxgene/3d_molecule_save/fine-tuning\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# num_folds = 5\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# kfold_order = \"sequential\"  # random | sequential\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# if kfold_order == \"random\":\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# load model and dictionary\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m model_backbone, dictionary \u001b[38;5;241m=\u001b[39m \u001b[43minit_unimol_backbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdict_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# build huggingface dataset from lmdb\u001b[39;00m\n\u001b[1;32m     21\u001b[0m lmdb_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/datasets/cellxgene/3d_molecule_data/1920-lib\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/SDL-LNP/model/active_learning/../unimol/hf_unimol.py:194\u001b[0m, in \u001b[0;36minit_unimol_backbone\u001b[0;34m(weight_path, dict_path, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_unimol_backbone\u001b[39m(weight_path, dict_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./dict.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    193\u001b[0m     parser \u001b[38;5;241m=\u001b[39m argparse\u001b[38;5;241m.\u001b[39mArgumentParser()\n\u001b[0;32m--> 194\u001b[0m     dictionary \u001b[38;5;241m=\u001b[39m \u001b[43mDictionary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdict_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     dictionary\u001b[38;5;241m.\u001b[39madd_symbol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[MASK]\u001b[39m\u001b[38;5;124m\"\u001b[39m, is_special\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    196\u001b[0m     args_unimol, unknown \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_known_args()\n",
      "File \u001b[0;32m~/miniconda3/envs/unimol/lib/python3.9/site-packages/unicore-0.0.1-py3.9-linux-x86_64.egg/unicore/data/dictionary.py:104\u001b[0m, in \u001b[0;36mDictionary.load\u001b[0;34m(cls, f)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads the dictionary from a text file with the format:\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    103\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[0;32m--> 104\u001b[0m \u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m d\n",
      "File \u001b[0;32m~/miniconda3/envs/unimol/lib/python3.9/site-packages/unicore-0.0.1-py3.9-linux-x86_64.egg/unicore/data/dictionary.py:117\u001b[0m, in \u001b[0;36mDictionary.add_from_file\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_from_file(fd)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m fnfe:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m fnfe\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeError\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect encoding detected in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, please \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrebuild the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(f)\n\u001b[1;32m    122\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/unimol/lib/python3.9/site-packages/unicore-0.0.1-py3.9-linux-x86_64.egg/unicore/data/dictionary.py:114\u001b[0m, in \u001b[0;36mDictionary.add_from_file\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fd:\n\u001b[1;32m    115\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_from_file(fd)\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m fnfe:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/fs01/home/haotian/SDL-LNP/model/unimol/dict.txt'"
     ]
    }
   ],
   "source": [
    "weight_name = \"baseline\"\n",
    "weight_mapping = {\n",
    "    \"baseline\": \"/scratch/ssd004/datasets/cellxgene/3d_molecule_save/weights/mol_pre_no_h_220816.pt\",\n",
    "    \"20240502-1907\": \"/datasets/cellxgene/3d_molecule_save/pretrain-20240502-1907/checkpoint_best.pt\",\n",
    "}\n",
    "weight_path = weight_mapping[weight_name]\n",
    "dict_path = \"/fs01/home/haotian/SDL-LNP/model/unimol/dict.txt\"\n",
    "cache_dir = \"/datasets/cellxgene/3d_molecule_data/cache\"\n",
    "output_path = \"/datasets/cellxgene/3d_molecule_save/fine-tuning\"\n",
    "# num_folds = 5\n",
    "# kfold_order = \"sequential\"  # random | sequential\n",
    "# if kfold_order == \"random\":\n",
    "#     kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "# else:\n",
    "#     kf = KFold(n_splits=num_folds, shuffle=False)\n",
    "\n",
    "# load model and dictionary\n",
    "model_backbone, dictionary = init_unimol_backbone(weight_path, dict_path=dict_path)\n",
    "\n",
    "# build huggingface dataset from lmdb\n",
    "lmdb_dir = Path(\"/datasets/cellxgene/3d_molecule_data/1920-lib\")\n",
    "train_data = load_dataset(\n",
    "    dictionary,\n",
    "    str(lmdb_dir / \"train.lmdb\"),\n",
    "    \"train\",\n",
    ")\n",
    "valid_data = load_dataset(\n",
    "    dictionary,\n",
    "    str(lmdb_dir / \"valid.lmdb\"),\n",
    "    \"valid\",\n",
    ")\n",
    "test_data = load_dataset(\n",
    "    dictionary,\n",
    "    str(lmdb_dir / \"test.lmdb\"),\n",
    "    \"test\",\n",
    ")\n",
    "\n",
    "hf_train_data = Dataset.from_generator(\n",
    "    lambda: train_data,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "hf_valid_data = Dataset.from_generator(\n",
    "    lambda: valid_data,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "hf_test_data = Dataset.from_generator(\n",
    "    lambda: test_data,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "combined_data = concatenate_datasets([hf_train_data, hf_valid_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first half of combined_data as the initial training set\n",
    "shards = 2\n",
    "initial_train_data = combined_data.shard(2, 0, contiguous=True, keep_in_memory=True)\n",
    "data_pool = combined_data.shard(2, 1, contiguous=True, keep_in_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In the active learning framework\n",
    "# data_pool = combined_data\n",
    "X_pool = data_pool\n",
    "y_pool = data_pool[\"target\"]\n",
    "X_test = hf_test_data\n",
    "y_test = hf_test_data[\"target\"]\n",
    "# # X_pool = X_pool[:1000]\n",
    "# # y_pool = y_pool[:1000]\n",
    "\n",
    "X_initial = initial_train_data\n",
    "y_initial = initial_train_data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import spearmanr\n",
    "\n",
    "# spearmanr(test_output.label_ids[0], test_output.predictions[0])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModALModelWrapper:\n",
    "    \"\"\"\n",
    "    Trainer class that is compatible with modAL active learning framework.\n",
    "    \"\"\"\n",
    "    # default model_config args\n",
    "    default_model_config = {\n",
    "        \"input_dim\":512,\n",
    "        \"inner_dim\":512,\n",
    "        \"num_classes\":1,\n",
    "        \"dropout\":0,\n",
    "        \"decoder_type\":\"mlp\",\n",
    "    }\n",
    "\n",
    "    default_training_args = {\n",
    "        \"output_dir\": None,\n",
    "        \"num_train_epochs\": 12,\n",
    "        \"per_device_train_batch_size\": 64,\n",
    "        \"per_device_eval_batch_size\": 256,\n",
    "        \"dataloader_num_workers\": 4,\n",
    "        \"remove_unused_columns\": False,\n",
    "        \"logging_dir\": \"./logs\",\n",
    "        \"fp16\": True,\n",
    "        \"logging_steps\": 100,\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"save_strategy\": \"steps\",\n",
    "        \"save_steps\": 500,\n",
    "        \"eval_steps\": 100,\n",
    "        \"report_to\": \"wandb\",\n",
    "        \"label_names\": [\"target\", \"smi_name\"],\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"optim\": \"adamw_torch\",\n",
    "        \"metric_for_best_model\": \"relaxed_spearman\",\n",
    "    }\n",
    "\n",
    "\n",
    "    def __init__(self, weight_path, dict_path, output_path, eval_dataset=None, **kwargs):\n",
    "        model_config_kwargs = self.default_model_config.copy()\n",
    "        model_config_kwargs.update(kwargs)\n",
    "        self.model_config = UniMolConfig(**model_config_kwargs)\n",
    "        self.default_training_args = self.default_training_args.copy()\n",
    "        self.default_training_args[\"output_dir\"] = output_path\n",
    "\n",
    "        self.model_backbone, self.dictionary = init_unimol_backbone(\n",
    "            weight_path, dict_path=dict_path\n",
    "        )\n",
    "        self.eval_dataset = eval_dataset\n",
    "\n",
    "    def _init_trainer(self, train_dataset, training_args):\n",
    "        training_arguments = self.default_training_args.copy()\n",
    "        training_arguments.update(training_args)\n",
    "        self.training_arguments = TrainingArguments(**training_arguments)\n",
    "\n",
    "        model = UniMol(self.model_backbone, self.model_config, self.dictionary)\n",
    "\n",
    "        optimizer = FusedAdam(\n",
    "            model.parameters(),\n",
    "            lr=1e-4,\n",
    "            eps=1e-6,\n",
    "            betas=(0.9, 0.99),\n",
    "        )\n",
    "\n",
    "        warmup_ratio = 0.06\n",
    "        training_steps = len(train_dataset) * training_arguments[\"num_train_epochs\"]\n",
    "        warmup_steps = int(training_steps * warmup_ratio)\n",
    "\n",
    "        scheduler = transformers.get_polynomial_decay_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=training_steps,\n",
    "        )\n",
    "\n",
    "        return Trainer(\n",
    "            model=model,\n",
    "            args=self.training_arguments,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=self.eval_dataset,\n",
    "            data_collator=collate_fn,\n",
    "            compute_metrics=compute_metrics,\n",
    "            tokenizer=None,\n",
    "            optimizers=(optimizer, scheduler),\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def model(self):\n",
    "        return self.trainer.model\n",
    "    \n",
    "    @property\n",
    "    def train_dataset(self):\n",
    "        return self.trainer.train_dataset\n",
    "    \n",
    "    @train_dataset.setter\n",
    "    def train_dataset(self, dataset):\n",
    "        self.trainer.train_dataset = dataset\n",
    "\n",
    "    def fit(self, X, y=None, **training_args):\n",
    "        \"\"\"\n",
    "        Fit the model with the given input data.\n",
    "        \"\"\"\n",
    "        self.trainer = self._init_trainer(X, training_args)\n",
    "        # self.trainer.train_dataset = X\n",
    "        self.trainer.train()\n",
    "\n",
    "    def predict(self, X, return_std=False):\n",
    "        \"\"\"\n",
    "        Predict the target values for the given input data.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"trainer\"):\n",
    "            raise ValueError(\"Model not trained yet. Usually you should call ModALModelWrapper.fit first.\")\n",
    "        predictions = self.trainer.predict(X)\n",
    "        smi_names = predictions.label_ids[1]\n",
    "        predictions = predictions.predictions[0]\n",
    "        if return_std:\n",
    "            # predictions have repeated smi_names, compute the std per smi_name\n",
    "            import pandas as pd\n",
    "\n",
    "            df = pd.DataFrame(\n",
    "                {\n",
    "                    \"smi_name\": smi_names,\n",
    "                    \"prediction\": predictions,\n",
    "                }\n",
    "            )\n",
    "            std = df.groupby(\"smi_name\").std().values\n",
    "            # map it back to length of predictions\n",
    "            std = np.array([std[smi_names == smi][0] for smi in smi_names])\n",
    "            assert len(std) == len(predictions)\n",
    "            return predictions, std\n",
    "\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "model_wrapper = ModALModelWrapper(weight_path, dict_path, output_path, eval_dataset=hf_test_data)\n",
    "# model_wrapper.fit(initial_train_data, num_train_epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='894' max='894' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [894/894 01:52, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearman</th>\n",
       "      <th>Relaxed Spearman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>23.489600</td>\n",
       "      <td>16.697338</td>\n",
       "      <td>0.193469</td>\n",
       "      <td>0.167758</td>\n",
       "      <td>0.168234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>14.175800</td>\n",
       "      <td>16.860138</td>\n",
       "      <td>0.530302</td>\n",
       "      <td>0.535511</td>\n",
       "      <td>0.536151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>13.776400</td>\n",
       "      <td>16.135302</td>\n",
       "      <td>0.628915</td>\n",
       "      <td>0.675216</td>\n",
       "      <td>0.676017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>13.175100</td>\n",
       "      <td>16.493685</td>\n",
       "      <td>0.640179</td>\n",
       "      <td>0.689035</td>\n",
       "      <td>0.689955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>12.135800</td>\n",
       "      <td>15.154257</td>\n",
       "      <td>0.671557</td>\n",
       "      <td>0.688011</td>\n",
       "      <td>0.689018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>11.506500</td>\n",
       "      <td>13.323022</td>\n",
       "      <td>0.651926</td>\n",
       "      <td>0.660493</td>\n",
       "      <td>0.661202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>11.285000</td>\n",
       "      <td>17.330334</td>\n",
       "      <td>0.664367</td>\n",
       "      <td>0.671380</td>\n",
       "      <td>0.672253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>10.507900</td>\n",
       "      <td>16.075773</td>\n",
       "      <td>0.645682</td>\n",
       "      <td>0.662008</td>\n",
       "      <td>0.663138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize ActiveLearner\n",
    "learner = ActiveLearner(\n",
    "    estimator=model_wrapper,\n",
    "    query_strategy=max_std_sampling,\n",
    "    X_training=X_initial,\n",
    "    y_training=y_initial,\n",
    "    num_train_epochs=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['src_tokens', 'src_coord', 'src_distance', 'src_edge_type', 'target', 'smi_name'],\n",
       "    num_rows: 9504\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the active learning loop\n",
    "n_queries = 10\n",
    "for idx in range(n_queries):\n",
    "    query_idx, query_instance = learner.query(X_pool, n_instances=100)\n",
    "    print(query_idx)\n",
    "    print(query_instance)\n",
    "    learner.teach(X_pool[query_idx], y_pool[query_idx], only_new=True, num_train_epochs=6)\n",
    "    # remove queried instance from pool\n",
    "    # X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "    # y_pool = np.delete(y_pool, query_idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualizing the data\n",
    "# with plt.style.context('seaborn-v0_8-bright'):\n",
    "#     plt.figure(figsize=(7, 7))\n",
    "#     plt.scatter(X, y, c='k')\n",
    "#     plt.title('Noisy absolute value function')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize the model\n",
    "\n",
    "# # five fold cross validation and init five different models\n",
    "\n",
    "# learner_list = []\n",
    "\n",
    "# # initializing the Committee\n",
    "# committee = CommitteeRegressor(\n",
    "#     learner_list=learner_list,\n",
    "#     query_strategy=max_std_sampling\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unimol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
